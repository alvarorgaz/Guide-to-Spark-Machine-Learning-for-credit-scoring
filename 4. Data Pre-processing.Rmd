---
title: "Guide to Spark Machine Learning for credit scoring"
author: "Álvaro Orgaz Expósito"
date: "29 June 2018"
output: html_document
---

# 4. DATA PRE-PROCESSING

Modify missing values found in the analysis:

- in numerical variables by 0

- in categorical variables by "Missing"

```{r}
data <- data %>% mutate(
  Postal_Code_ASNEF=ifelse(is.na(Postal_Code_ASNEF),0,Postal_Code_ASNEF),
  Additional_Income=ifelse(is.na(Additional_Income),0,Additional_Income),
  Partner_Income=ifelse(is.na(Partner_Income),0,Partner_Income),
  Rent=ifelse(is.na(Rent),0,Rent),
  Mortgage=ifelse(is.na(Mortgage),0,Mortgage),
  Profession_Code=ifelse(is.na(Profession_Code),"Missing",Profession_Code)
  )
```

Create the list with valid levels for categorical variables (excluding categories with <5%)
```{r}
valid_levels <- list(
  Levels_Application_Hour_Group=
    c("[23H, 7H)","[7H, 20H)","[20H, 23H)","OTHERS"),
  Levels_Application_Week_Day=
    c("1","2","3","4","5","6","7","OTHERS"),
  Levels_Gender=
    c("MALE","FEMALE","OTHERS"),
  Levels_Profession_Sector=
    c("PRIVATE_SECTOR","PUBLIC_SECTOR","OTHERS"),
  Levels_Contract_Type=
    c("PERMANENT","PENSION","OTHERS"),
  Levels_People_in_Household=
    c("0","1","2","OTHERS"),
  Levels_Num_Ongoing_Credits=
    c("0","1","2","3","OTHERS"),
  Levels_Marital_Status=
    c("DIVORCED","SINGLE","COHABITING","MARRIED","OTHERS"),
  Levels_Province=
    c("Madrid","Barcelona","Asturias (Oviedo)","OTHERS"),
  Levels_Profession_Code=
    c("OPERATOR", "ADMINISTRATIVE","TECHNICIAN","MIDDLEGRADEMANAGER","RETIREMENT","OTHERS"),
  Levels_Purpose=
    c("LIQUIDITY","HOMEIMPROVEMENT","DEBTS","FURNITURE_AND_APPLIANCES","USEDCAR","MEDICALCARE",
      "VACATION","OTHERS"),
  Levels_Housing_Type=
    c("THIRD_PARTY_PROVIDED_LODGING","HOME_OWNERSHIP_WITHOUT_MORTGAGE","TENANT",
      "HOME_OWNERSHIP_WITH_MORTGAGE","OTHERS")
  )
```

Change outliers categories by OTHERS
```{r}
data <- data %>% mutate(
  Purpose=
    ifelse(Purpose %in% valid_levels[["Levels_Purpose"]],Purpose,"OTHERS"),
  Gender=
    ifelse(Gender %in% valid_levels[["Levels_Gender"]],Gender,"OTHERS"),
  Housing_Type=
    ifelse(Housing_Type %in% valid_levels[["Levels_Housing_Type"]],Housing_Type,"OTHERS"),
  Province=
    ifelse(Province %in% valid_levels[["Levels_Province"]],Province,"OTHERS"),
  Marital_Status=
    ifelse(Marital_Status %in% valid_levels[["Levels_Marital_Status"]],Marital_Status,"OTHERS"),
  Profession_Code=
    ifelse(Profession_Code %in% valid_levels[["Levels_Profession_Code"]],Profession_Code,"OTHERS"),
  Contract_Type=
    ifelse(Contract_Type %in% valid_levels[["Levels_Contract_Type"]],Contract_Type,"OTHERS"),
  Profession_Sector=
    ifelse(Profession_Sector %in% valid_levels[["Levels_Profession_Sector"]],Profession_Sector,
           "OTHERS"),
  Application_Week_Day=
    ifelse(Application_Week_Day %in% valid_levels[["Levels_Application_Week_Day"]],
           Application_Week_Day,"OTHERS"),
  People_in_Household=
    ifelse(People_in_Household %in% valid_levels[["Levels_People_in_Household"]],People_in_Household,
           "OTHERS"),
  Num_Ongoing_Credits=
    ifelse(Num_Ongoing_Credits %in% valid_levels[["Levels_Num_Ongoing_Credits"]],Num_Ongoing_Credits,
           "OTHERS"),
  Application_Hour_Group=
    ifelse(Application_Hour_Group %in% valid_levels[["Levels_Application_Hour_Group"]],
           Application_Hour_Group,"OTHERS")
  )
```

Notes:

- **N6.** In the case that you need to convert all categorical variables to numerical with an integer index, the necessary Spark R code could be:

```{r}
for(i in categorical[-which(categorical=="Contract_ID" | categorical=="Defaulted")]){
  label_i  <- as.vector(valid_levels[[paste0("Levels_",i)]])
  data <- data %>% ft_string_indexer_model(input_col=i,output_col=paste0(i,"_INDEXED"),labels=label_i)
}
```

Save the Spark data frame with the pre-processed data in the Spark cluster
```{r}
data <- copy_to(sc_sparklyr,data,overwrite=T)
```

Save the Spark data frame with the pre-processed data in a local file with format Parquet
```{r}
spark_write_parquet(data,"data.parquet")
```